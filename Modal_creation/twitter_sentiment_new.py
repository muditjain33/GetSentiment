# -*- coding: utf-8 -*-
"""Twitter_Sentiment_New.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pANF9YSj-aoMlTI_VuWtcsl6nAVF31p4

## IMPORTING THE DATA
"""

import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import warnings

from google.colab import drive
drive.mount('/content/drive')

import os
os. chdir('drive')
os. chdir('MyDrive')

os. chdir('dataset')

train = pd.read_csv('train_tweet.csv')
train1 = pd.read_csv('neg_words.csv')
train2 = pd.read_csv('neg_phrase.csv')
train3 = pd.read_csv('pos_words.csv')
train4 = pd.read_csv('pos_phrase.csv')
test = pd.read_csv('test_tweets.csv')

print(train.shape)
print(test.shape)

train1.head()

train2.head()

train4.head()

train3.head()

IMDBtrain=pd.read_csv('IMDB_dataset.csv')

IMDBtrain.head()

def label(element):
    if element=="positive":
        return 0
    elif element=="negative":
        return 1
    else:
        return element

IMDBtrain['sentiment']=IMDBtrain['sentiment'].apply(label)

IMDBtrain.size

IMDBtrain['review'] = IMDBtrain['review'].str.replace(r'<[^<>]*>', '', regex=True)

IMDBtrain.head()

train.head()

test.head()

train.isnull().any()
test.isnull().any()

# checking out the negative comments from the train set

train[train['label'] == 0].head(10)

# checking out the postive comments from the train set 

train[train['label'] == 1].head(10)

"""## VISUALIZATION"""

train['label'].value_counts().plot.bar(color = 'pink', figsize = (6, 4))

# checking the distribution of tweets in the data

length_train = train['tweet'].str.len().plot.hist(color = 'pink', figsize = (6, 4))
length_test = test['tweet'].str.len().plot.hist(color = 'orange', figsize = (6, 4))

# adding a column to represent the length of the tweet

train['len'] = train['tweet'].str.len()
test['len'] = test['tweet'].str.len()

train.head(10)

train.groupby('label').describe()

train.groupby('len').mean()['label'].plot.hist(color = 'black', figsize = (6, 4),)
plt.title('variation of length')
plt.xlabel('Length')
plt.show()

"""## PREPROCESSING"""

from sklearn.feature_extraction.text import CountVectorizer


cv = CountVectorizer(stop_words = 'english',max_features=20000)
words = cv.fit_transform(train.tweet)
sum_words = words.sum(axis=0)

words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)

frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])

frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'blue')
plt.title("Most Frequently Occuring Words - Top 30")
# print(cv.vocabulary_)

from wordcloud import WordCloud

wordcloud = WordCloud(background_color = 'white', width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))

plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.title("WordCloud - Vocabulary from Reviews", fontsize = 22)

normal_words =' '.join([text for text in train['tweet'][train['label'] == 0]])

wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(normal_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title('The Neutral Words')
plt.show()

negative_words =' '.join([text for text in train['tweet'][train['label'] == 1]])

wordcloud = WordCloud(background_color = 'cyan', width=800, height=500, random_state = 0, max_font_size = 110).generate(negative_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title('The Negative Words')
plt.show()

# collecting the hashtags

def hashtag_extract(x):
    hashtags = []
    
    for i in x:
        ht = re.findall(r"#(\w+)", i)
        hashtags.append(ht)

    return hashtags

# extracting hashtags from non racist/sexist tweets
HT_regular = hashtag_extract(train['tweet'][train['label'] == 0])

# extracting hashtags from racist/sexist tweets
HT_negative = hashtag_extract(train['tweet'][train['label'] == 1])

# unnesting list
HT_regular = sum(HT_regular,[])
HT_negative = sum(HT_negative,[])

a = nltk.FreqDist(HT_regular)
d = pd.DataFrame({'Hashtag': list(a.keys()),
                  'Count': list(a.values())})

# selecting top 20 most frequent hashtags     
d = d.nlargest(columns="Count", n = 20) 
plt.figure(figsize=(16,5))
ax = sns.barplot(data=d, x= "Hashtag", y = "Count")
ax.set(ylabel = 'Count')
plt.show()

a = nltk.FreqDist(HT_negative)
d = pd.DataFrame({'Hashtag': list(a.keys()),
                  'Count': list(a.values())})

# selecting top 20 most frequent hashtags     
d = d.nlargest(columns="Count", n = 20) 
plt.figure(figsize=(16,5))
ax = sns.barplot(data=d, x= "Hashtag", y = "Count")
ax.set(ylabel = 'Count')
plt.show()

# tokenizing the words present in the training set
tokenized_tweet = train['tweet'].apply(lambda x: x.split()) 

# importing gensim
import gensim

# creating a word to vector model
model_w2v = gensim.models.Word2Vec(
            tokenized_tweet,
            vector_size=200, # desired no. of features/independent variables 
            window=5, # context window size
            min_count=4, 
            sg = 1, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 2, # no.of cores
            seed = 34)

model_w2v.train(tokenized_tweet, total_examples= len(train['tweet']), epochs=20)

model_w2v.wv.most_similar(positive = "dinner")

model_w2v.wv.most_similar(positive = "cancer")

model_w2v.wv.most_similar(positive = "apple")

model_w2v.wv.most_similar(negative = "hate")

"""## Test and Training set Making """

# removing unwanted patterns from the data

import re
import nltk

nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

train_corpus = []

for i in range(0, 31962):
  review = re.sub('[^a-zA-Z]', ' ', train['tweet'][i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
  
  # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  train_corpus.append(review)

test_corpus = []

for i in range(0, 17197):
  review = re.sub('[^a-zA-Z]', ' ', test['tweet'][i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
  
  # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  test_corpus.append(review)

IMDBtrain.shape

IMDB_corpus = []

for i in range(0, 50000):
  review = re.sub('[^a-zA-Z]', ' ', IMDBtrain['review'][i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
  
  # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  IMDB_corpus.append(review)

ytlist=["good","bad","distrack"]
yt1=[]
for i in range(0, len(ytlist)):
  review = re.sub('[^a-zA-Z]', ' ', ytlist[i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
  
  # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  yt1.append(review)

def preprocessing(ytlist):
    yt1=[]
    for i in range(0, len(ytlist)):
        review = re.sub('[^a-zA-Z]', ' ', ytlist[i])
        review = review.lower()
        review = review.split()
        ps = PorterStemmer()
        review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
        review = ' '.join(review)
        yt1.append(review)
    return yt1

train11=preprocessing(train1['sss'])
train22=preprocessing(train2['sss'])
train33=preprocessing(train3['sss'])
train44=preprocessing(train4['sss'])

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 10000)
cv.fit_transform(train11+train22+train33+train44)

print((cv.vocabulary_))

def changetonum(word):
    try:
#         print(cv.vocabulary_[word])
        return cv.vocabulary_[word]
    except:
        return -1

train_corpus1=[]
for i in train_corpus:
    lq=[]
    for j in i.split():
        if(changetonum(j)!=-1):
            lq.append(changetonum(j))
    train_corpus1.append(lq)
print(train_corpus1[0])

test_corpus1=[]
for i in test_corpus:
    lq=[]
    for j in i.split():
        if(changetonum(j)!=-1):
            lq.append(changetonum(j))
    test_corpus1.append(lq)
print(test_corpus1[0])

IMDB_corpus1=[]
for i in IMDB_corpus:
    lq=[]
    for j in i.split():
        if(changetonum(j)!=-1):
            lq.append(changetonum(j))
    IMDB_corpus1.append(lq)
print(IMDB_corpus1[0])

yt_corpus1=[]
for i in yt1:
    lq=[]
    for j in i.split():
        if(changetonum(j)!=-1):
            lq.append(changetonum(j))
    yt_corpus1.append(lq)
print(yt_corpus1[0])

def preprocessing2(yt1):
    yt_corpus1=[]
    for i in yt1:
        lq=[]
        for j in i.split():
            if(changetonum(j)!=-1):
                lq.append(changetonum(j))
        yt_corpus1.append(lq)
    return yt_corpus1

train11=preprocessing2(train11)
train22=preprocessing2(train22)
train33=preprocessing2(train33)
train44=preprocessing2(train44)

print(len(cv.vocabulary_))

import numpy as np
def vectorize_sentences(sentences,dim=10000):
    outputs = np.zeros((len(sentences),dim))
    for i,j in enumerate(sentences):
        for k in j:
            outputs[i,k] = 1
    return outputs

train_corpus1=vectorize_sentences(train_corpus1)

test_corpus1=vectorize_sentences(test_corpus1)

yt_corpus1=vectorize_sentences(yt_corpus1)

IMDB_corpus1=vectorize_sentences(IMDB_corpus1)

yt_corpus1.shape

train11=vectorize_sentences(train11)
train22=vectorize_sentences(train22)
train33=vectorize_sentences(train33)
train44=vectorize_sentences(train44)

train['label']

"""## Testing different models and evaluating"""

# splitting the training data into train and valid sets

from sklearn.model_selection import train_test_split

x_train, x_valid, y_train, y_valid = train_test_split(train_corpus1, train['label'], test_size = 0.25, random_state = 42)

print(x_train.shape)
print(x_valid.shape)
print(y_train.shape)
print(y_valid.shape)

from sklearn.svm import SVC

model = SVC()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("f1 score :", f1_score(y_valid, y_pred))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("f1 score :", f1_score(y_valid, y_pred))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("f1 score :", f1_score(y_valid, y_pred))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score

model = RandomForestClassifier()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("F1 score :", f1_score(y_valid, y_pred))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score


np.concatenate((train11,train22,train33,train44), axis=0)
np.concatenate((train1['1'],train2['1'],train3['0'],train4['0']), axis=0)
model = RandomForestClassifier()
model.fit(train11,train1['1'])

from sklearn.naive_bayes import GaussianNB
gmodel = GaussianNB()
gmodel.fit(train_corpus1,train['label'])

import joblib
joblib.dump(model, 'model.pkl')

joblib.dump(gmodel, 'Gaussmodel.pkl')

model_from_joblib = joblib.load('Gaussmodel.pkl')
sum(model_from_joblib.predict(test_corpus1))

test_corpus1.shape

x_train.shape

x_valid.shape

381+1674

sum(train['label'])

